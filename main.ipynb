{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML4HPC: Ensemble of Forecasters\n",
    "\n",
    "### Team Members:\n",
    "- Luca Venerando Greco\n",
    "- Bice Marzagora\n",
    "- Elia Vaglietti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "In this notebook, we start by importing several essential libraries that are used throughout the workflow:\n",
    "\n",
    "- `os`: Provides functions for interacting with the operating system, such as creating directories and handling file paths.\n",
    "- `sys`: Provides access to some variables used or maintained by the Python interpreter and to functions that interact strongly with the interpreter.\n",
    "- `matplotlib.pyplot`: A plotting library used for creating static, animated, and interactive visualizations in Python.\n",
    "- `numpy`: A fundamental package for scientific computing with Python, used for working with arrays and matrices.\n",
    "- `time`: Provides various time-related functions.\n",
    "- `tqdm`: A library for creating progress bars and progress meters.\n",
    "\n",
    "These libraries are crucial for tasks such as data manipulation, visualization, and managing the execution of jobs in a high-performance computing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 16:35:44.802400: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-02 16:35:44.808143: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-02 16:35:44.822348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733153744.842786 2227458 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733153744.849396 2227458 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 16:35:44.876063: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import time\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Setup and Job Configuration\n",
    "\n",
    "We now set up the necessary directories and define the job configurations. Specifically, we create folders for storing charts, data, and logs if they do not already exist. We also define the number of forecasters and nodes for different scaling tests.\n",
    "\n",
    "If no new data is needed, set the `GENERATE_DATA` variable to `False` to skip the data generation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "charts_folder = \"charts\"\n",
    "data_folder = \"data\"\n",
    "logs_folder = \"logs\"\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "if not os.path.exists(logs_folder):\n",
    "    os.makedirs(logs_folder)\n",
    "\n",
    "if not os.path.exists(charts_folder):\n",
    "    os.makedirs(charts_folder)\n",
    "\n",
    "ten_nodes = 10\n",
    "strong_scaling_nodes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Number of nodes to test\n",
    "weak_scaling_nodes   = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Number of nodes to test\n",
    "\n",
    "n_runs = 30\n",
    "\n",
    "GENERATE_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Submission Function\n",
    "\n",
    "We define a function `submit_job` that handles the submission of jobs to the scheduler. This function takes the number of nodes, the number of forecasters, and a job name as input parameters. It creates the necessary directories for storing data and logs, reads a template launch script, formats it with the provided parameters, writes the formatted script to a file, and submits the job using the `sbatch` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(launch_file, num_nodes, job_name, num_epochs):\n",
    "    num_tasks_per_node = 128\n",
    "\n",
    "    if num_tasks_per_node > 128:\n",
    "        print(\"The number of tasks per node should be less than or equal to 128\")\n",
    "        exit(1)\n",
    "\n",
    "    if not os.path.exists(f\"{data_folder}/{job_name}\"):\n",
    "        os.makedirs(f\"{data_folder}/{job_name}\")\n",
    "\n",
    "    if not os.path.exists(f\"{logs_folder}/{job_name}\"):\n",
    "        os.makedirs(f\"{logs_folder}/{job_name}\")\n",
    "\n",
    "    with open(launch_file, 'r') as file:\n",
    "        launch_script = file.read()\n",
    "\n",
    "    launch_script = launch_script.format(\n",
    "        num_nodes=num_nodes,\n",
    "        num_tasks_per_node=num_tasks_per_node,\n",
    "        current_dir=current_dir,\n",
    "        world_size=num_nodes*num_tasks_per_node,\n",
    "        num_epochs=num_epochs,\n",
    "        data_folder=f\"{data_folder}/{job_name}\",\n",
    "        logs_folder=f\"{logs_folder}/{job_name}\"\n",
    "    )\n",
    "\n",
    "    script_filename = f\"{logs_folder}/{job_name}/{launch_file.split('/')[-1]}\"\n",
    "    with open(script_filename, \"w\") as script_file:\n",
    "        script_file.write(launch_script)\n",
    "\n",
    "    os.system(f\"sbatch {script_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Test Functions\n",
    "\n",
    "In the following sections, we define functions to run different scalability tests. These functions will help us automate the process of submitting jobs for one million forecasters, strong scaling, and weak scaling tests. Each function will generate a unique job name, submit the job using the `submit_job` function, and return the job names for tracking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_nodes_test():\n",
    "    job_names = []\n",
    "    for i in range(n_runs):\n",
    "        run_dir = f\"{data_folder}/ten_nodes_test/run_{i}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.makedirs(run_dir)\n",
    "        \n",
    "        job_name = f\"/ten_nodes_test/run_{i}\"\n",
    "\n",
    "        submit_job(ten_nodes, job_name)\n",
    "\n",
    "        job_names.append(job_name)\n",
    "\n",
    "    return job_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_test():\n",
    "    job_names = []\n",
    "    for i in range(n_runs):\n",
    "        run_dir = f\"{data_folder}/gpu_test/run_{i}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.makedirs(run_dir)\n",
    "        \n",
    "        job_name = f\"/gpu_test/run_{i}\"\n",
    "\n",
    "        submit_job(\"launchers/launch_gpu.sh\", 1, job_name, 10)\n",
    "\n",
    "        job_names.append(job_name)\n",
    "\n",
    "    return job_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strong_scaling():\n",
    "    job_names = []\n",
    "    for run in range(1):\n",
    "        for num_nodes in strong_scaling_nodes:\n",
    "            job_name = f\"/strong_scaling/run_{run}/nodes_{num_nodes}\"\n",
    "            submit_job(num_nodes, job_name)\n",
    "            job_names.append(job_name)\n",
    "    return job_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weak_scaling():\n",
    "#     job_names = []\n",
    "#     for run in range(1):\n",
    "#         for num_nodes in weak_scaling_nodes:\n",
    "#             job_name = f\"/weak_scaling/run_{run}/nodes_{num_nodes}_forecasters_{weak_scaling_forecasters*num_nodes}\"\n",
    "#             submit_job(num_nodes, job_name)\n",
    "#             job_names.append(job_name)\n",
    "#     return job_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_test():\n",
    "    job_names = []\n",
    "    for i in range(n_runs):\n",
    "        run_dir = f\"{data_folder}/baseline_test/run_{i}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.makedirs(run_dir)\n",
    "        \n",
    "        job_name = f\"/baseline_test/run_{i}\"\n",
    "\n",
    "        submit_job(\"launchers/launch_baseline.sh\", 1, job_name, 10)\n",
    "\n",
    "        job_names.append(job_name)\n",
    "\n",
    "    return job_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting for jobs\n",
    "\n",
    "Now we wait for all the jobs to complete, in the meantime the `tqdm` progress bar will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: access1.iris-cluster.uni.lux\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "print(f\"Hostname: {hostname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3692889\n",
      "Submitted batch job 3692890\n",
      "Submitted batch job 3692891\n",
      "Submitted batch job 3692892\n",
      "Submitted batch job 3692893\n",
      "Submitted batch job 3692894\n",
      "Submitted batch job 3692895\n",
      "Submitted batch job 3692896\n",
      "Submitted batch job 3692897\n",
      "Submitted batch job 3692898\n",
      "Submitted batch job 3692899\n",
      "Submitted batch job 3692900\n",
      "Submitted batch job 3692901\n",
      "Submitted batch job 3692902\n",
      "Submitted batch job 3692903\n",
      "Submitted batch job 3692904\n",
      "Submitted batch job 3692905\n",
      "Submitted batch job 3692906\n",
      "Submitted batch job 3692907\n",
      "Submitted batch job 3692908\n",
      "Submitted batch job 3692909\n",
      "Submitted batch job 3692910\n",
      "Submitted batch job 3692911\n",
      "Submitted batch job 3692912\n",
      "Submitted batch job 3692913\n",
      "Submitted batch job 3692914\n",
      "Submitted batch job 3692915\n",
      "Submitted batch job 3692916\n",
      "Submitted batch job 3692917\n",
      "Submitted batch job 3692918\n",
      "Waiting for jobs to finish...\n",
      "['/gpu_test/run_0', '/gpu_test/run_1', '/gpu_test/run_2', '/gpu_test/run_3', '/gpu_test/run_4', '/gpu_test/run_5', '/gpu_test/run_6', '/gpu_test/run_7', '/gpu_test/run_8', '/gpu_test/run_9', '/gpu_test/run_10', '/gpu_test/run_11', '/gpu_test/run_12', '/gpu_test/run_13', '/gpu_test/run_14', '/gpu_test/run_15', '/gpu_test/run_16', '/gpu_test/run_17', '/gpu_test/run_18', '/gpu_test/run_19', '/gpu_test/run_20', '/gpu_test/run_21', '/gpu_test/run_22', '/gpu_test/run_23', '/gpu_test/run_24', '/gpu_test/run_25', '/gpu_test/run_26', '/gpu_test/run_27', '/gpu_test/run_28', '/gpu_test/run_29']\n"
     ]
    }
   ],
   "source": [
    "all_jobs_to_wait = []\n",
    "\n",
    "if GENERATE_DATA:\n",
    "    # if hostname contains \"aion\":\n",
    "    if \"aion\" in hostname:\n",
    "        all_jobs_to_wait.extend(baseline_test())\n",
    "        # all_jobs_to_wait.extend(ten_nodes_test())\n",
    "        # all_jobs_to_wait.extend(strong_scaling())\n",
    "        # all_jobs_to_wait.extend(weak_scaling())\n",
    "    elif \"iris\" in hostname:\n",
    "        all_jobs_to_wait.extend(gpu_test())\n",
    "\n",
    "    print(\"Waiting for jobs to finish...\")\n",
    "    print(all_jobs_to_wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_name in tqdm(all_jobs_to_wait):\n",
    "    while not os.path.exists(f\"{data_folder}/{job_name}/timings.txt\"):\n",
    "        time.sleep(10)  # Poll every 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Analysis\n",
    "\n",
    "In this section, we analyze the execution times for the one million forecasters test. We read the timing data from the generated files, calculate the mean and standard deviation of the execution times, and create a dataframe to summarize the results.\n",
    "\n",
    "The dataframe includes the following columns:\n",
    "- **Run**: The run identifier.\n",
    "- **Timing**: The total execution time for each run.\n",
    "- **Aggregate sum CPU time**: The sum of CPU times across all ranks for each run.\n",
    "- **Aggregate mean CPU time**: The mean CPU time across all ranks for each run.\n",
    "\n",
    "We then print the dataframe and the calculated mean and standard deviation of the execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_mean_and_std_of_times(job_name):\n",
    "    timings = []\n",
    "    cpu_times = []\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        with open(f\"{data_folder}/{job_name}/run_{i}/timings.txt\", \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            timings.append(float(lines[0].lstrip(\"Real time:\")))\n",
    "            cpu_times.append(float(lines[1].lstrip(\"CPU time:\")))\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "        'Run': [f'run_{i}' for i in range(n_runs)],\n",
    "        'Timing': timings,\n",
    "        'CPU Time': cpu_times\n",
    "    })\n",
    "\n",
    "    mean_timing = df['Timing'].mean()\n",
    "    std_timing = df['Timing'].std()\n",
    "\n",
    "    return df, mean_timing, std_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Test DataFrame:\n",
      "       Run      Timing    CPU Time\n",
      "0    run_0  36535.3858  47866.0677\n",
      "1    run_1  28292.5568  39011.7775\n",
      "2    run_2  37057.9193  48107.5929\n",
      "3    run_3  28184.3531  39052.3030\n",
      "4    run_4  36291.9042  47296.5360\n",
      "5    run_5  36207.7599  47648.9669\n",
      "6    run_6  28130.3158  39049.0143\n",
      "7    run_7  35866.5569  47147.0061\n",
      "8    run_8  36473.8846  47676.1727\n",
      "9    run_9  28066.5364  38949.7274\n",
      "10  run_10  35993.3417  46888.1049\n",
      "11  run_11  36634.2409  47894.0841\n",
      "12  run_12  28108.6109  39005.7201\n",
      "13  run_13  36261.7977  47453.9584\n",
      "14  run_14  28018.8692  38841.6865\n",
      "15  run_15  36855.7129  47627.9897\n",
      "16  run_16  28705.5709  39749.2466\n",
      "17  run_17  36283.8998  47314.1508\n",
      "18  run_18  29007.5645  40200.9690\n",
      "19  run_19  28989.4414  40092.6941\n",
      "20  run_20  35870.3802  46857.8725\n",
      "21  run_21  29645.9053  41005.6306\n",
      "22  run_22  28750.2661  39820.2386\n",
      "23  run_23  29579.6151  40953.0376\n",
      "24  run_24  36011.0643  47100.4461\n",
      "25  run_25  28775.6436  39838.9675\n",
      "26  run_26  29742.9457  41040.6240\n",
      "27  run_27  38256.1574  49473.2716\n",
      "28  run_28  28753.5064  39810.7783\n",
      "29  run_29  29461.2486  40763.0873\n",
      "Mean Timing: 32360.431846666666, Std Timing: 3957.1902472873585\n",
      "Baseline Test DataFrame:\n",
      "       Run       Timing     CPU Time\n",
      "0    run_0  136340.9247  246832.8100\n",
      "1    run_1  134690.3076  247271.6793\n",
      "2    run_2  135923.6782  247180.0511\n",
      "3    run_3  134281.5256  242814.1275\n",
      "4    run_4  134805.1593  246774.6362\n",
      "5    run_5  133663.2371  245110.9290\n",
      "6    run_6  134967.6335  244635.5203\n",
      "7    run_7  135114.2733  245383.7379\n",
      "8    run_8  134620.4543  247206.0612\n",
      "9    run_9  135542.0134  251356.8831\n",
      "10  run_10  135883.5995  245663.4275\n",
      "11  run_11  132265.3103  241963.0318\n",
      "12  run_12  136498.4670  251740.6819\n",
      "13  run_13  133365.5851  246336.3348\n",
      "14  run_14  134577.8503  246254.1681\n",
      "15  run_15  137961.5316  253232.4258\n",
      "16  run_16  135231.6456  242708.1886\n",
      "17  run_17  133829.4003  248380.0792\n",
      "18  run_18  134863.8971  241521.3884\n",
      "19  run_19  132966.3792  239938.3462\n",
      "20  run_20  133581.1038  244839.0390\n",
      "21  run_21  137745.7535  251341.1993\n",
      "22  run_22  134638.3989  244491.6112\n",
      "23  run_23  135834.8343  247197.0840\n",
      "24  run_24  135584.8482  246002.5610\n",
      "25  run_25  133750.3068  239941.1753\n",
      "26  run_26  133129.6060  245324.1302\n",
      "27  run_27  133897.6576  243047.9927\n",
      "28  run_28  135282.3856  246062.6539\n",
      "29  run_29  136367.3446  246836.6430\n",
      "Mean Timing: 134906.83707666665, Std Timing: 1338.1616697659706\n",
      "Speedup: 4.168882470910626\n"
     ]
    }
   ],
   "source": [
    "# Get the dataframe for GPU test\n",
    "df_gpu, mean_timing_gpu, std_timing_gpu = get_mean_and_std_of_times(\"gpu_test\")\n",
    "print(\"GPU Test DataFrame:\")\n",
    "print(df_gpu)\n",
    "print(f\"Mean Timing: {mean_timing_gpu}, Std Timing: {std_timing_gpu}\")\n",
    "\n",
    "# Get the dataframe for Baseline test\n",
    "df_baseline, mean_timing_baseline, std_timing_baseline = get_mean_and_std_of_times(\"baseline_test\")\n",
    "print(\"Baseline Test DataFrame:\")\n",
    "print(df_baseline)\n",
    "print(f\"Mean Timing: {mean_timing_baseline}, Std Timing: {std_timing_baseline}\")\n",
    "\n",
    "# print mean speedup\n",
    "speedup = mean_timing_baseline / mean_timing_gpu\n",
    "print(f\"Speedup: {speedup}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Scalability Test\n",
    "\n",
    "In this section, we analyze the execution times for the strong scalability test. We have already submitted jobs for different numbers of nodes and collected the execution times. The results are plotted on a logarithmic scale to better visualize the differences in execution times as the number of nodes increases.\n",
    "\n",
    "The strong scalability test helps us understand how the execution time decreases as we increase the number of nodes while keeping the problem size constant. Ideally, the execution time should decrease proportionally with the increase in the number of nodes, indicating efficient parallelization and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_times_strong_scaling = []\n",
    "\n",
    "# Submit jobs for each test configuration\n",
    "for num_nodes in strong_scaling_nodes:\n",
    "    execution_time_file = f\"{data_folder}/strong_scaling/run_0/nodes_{num_nodes}/timings.txt\"\n",
    "\n",
    "    with open(execution_time_file, \"r\") as f:\n",
    "        line = f.readline().strip()\n",
    "        execution_time = float(line.replace(\"Total execution time: \", \"\"))\n",
    "    execution_times_strong_scaling.append(execution_time)\n",
    "    print(f\"Execution time for {num_nodes} nodes: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.yscale('log')\n",
    "plt.plot(strong_scaling_nodes, execution_times_strong_scaling, label='Strong Scaling', marker='o')\n",
    "plt.xlabel('Number of Nodes')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Strong Scalability Test')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(f\"{charts_folder}/scalability_plot.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Scalability Test\n",
    "\n",
    "In this section, we analyze the execution times for the weak scalability test. \n",
    "\n",
    "The weak scalability test helps us understand how the execution time changes as we increase the number of nodes while keeping the workload per node constant. Ideally, the execution time should remain constant with the increase in the number of nodes, indicating efficient parallelization and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution_times_weak_scaling = []\n",
    "\n",
    "# # Submit jobs for each test configuration\n",
    "# for num_nodes in weak_scaling_nodes:\n",
    "#     execution_time_file = f\"{data_folder}/weak_scaling/run_0/nodes_{num_nodes}_forecasters_{weak_scaling_forecasters*num_nodes}/timings.txt\"\n",
    "\n",
    "#     with open(execution_time_file, \"r\") as f:\n",
    "#         line = f.readline().strip()\n",
    "#         execution_time = float(line.replace(\"Total execution time: \", \"\"))\n",
    "#     execution_times_weak_scaling.append(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot the weak scalability results\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(weak_scaling_nodes, execution_times_weak_scaling, label='Weak Scaling', marker='o')\n",
    "# plt.xlabel(\"Number of Nodes\")\n",
    "# plt.ylabel(\"Execution Time (seconds)\")\n",
    "# plt.title(\"Weak Scalability Test\")\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f\"{charts_folder}/weak_scalability.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we have successfully set up and executed a series of scalability tests for an ensemble of forecasters in a high-performance computing environment. We started by importing the necessary libraries and setting up the directory structure. We then defined functions to submit jobs for one million forecasters, strong scaling, and weak scaling tests.\n",
    "\n",
    "We visualized the data generated from the one million forecasters test, analyzing the distribution of forecast, weights, and biases. The histograms confirmed that these variables are normally distributed, as expected.\n",
    "\n",
    "The strong scalability test demonstrated how the execution time decreases with an increasing number of nodes, indicating efficient parallelization. The weak scalability test showed that the execution time remains relatively constant as the number of nodes increases, suggesting good resource utilization.\n",
    "\n",
    "Overall, these tests provide valuable insights into the performance and scalability of our forecasting ensemble, helping us optimize and improve our high-performance computing workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
