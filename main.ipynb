{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML4HPC: CNN\n",
    "\n",
    "### Team Members:\n",
    "- Luca Venerando Greco\n",
    "- Bice Marzagora\n",
    "- Elia Vaglietti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "### Library Descriptions\n",
    "\n",
    "1. **NumPy (`np`)**:\n",
    "   - A fundamental package for scientific computing with Python. It provides support for arrays, matrices, and many mathematical functions to operate on these data structures.\n",
    "\n",
    "2. **Matplotlib (`plt`)**:\n",
    "   - A plotting library for creating static, animated, and interactive visualizations in Python. It is widely used for generating plots, histograms, bar charts, and other types of graphs.\n",
    "\n",
    "3. **Keras (`mnist`)**:\n",
    "   - A high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. The `mnist` module provides access to the MNIST dataset, a large database of handwritten digits commonly used for training various image processing systems.\n",
    "\n",
    "4. **JAX (`jax`, `jnp`, `grad`)**:\n",
    "   - A library for high-performance machine learning research. It provides NumPy-like API (`jax.numpy` or `jnp`) with automatic differentiation (`grad`), GPU/TPU acceleration, and just-in-time compilation to optimize performance.\n",
    "\n",
    "5. **Time (`time`)**:\n",
    "   - A standard Python library for time-related functions. It provides various time-related functions such as getting the current time, measuring the execution time of code, and more.\n",
    "\n",
    "6. **OS (`os`)**:\n",
    "   - A standard Python library for interacting with the operating system. It provides functions to interact with the file system, manage directories, and handle environment variables.\n",
    "\n",
    "7. **TQDM (`tqdm`)**:\n",
    "   - A library for creating progress bars in Python. It is useful for tracking the progress of loops and long-running operations, providing a visual indication of progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 23:27:38.515255: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-02 23:27:38.519489: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-02 23:27:38.530299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733178458.547251 2830939 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733178458.552997 2830939 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 23:27:38.572671: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Setup and Job Configuration\n",
    "\n",
    "We now set up the necessary directories and define the job configurations. Specifically, we create folders for storing data and logs, if they do not already exist.\n",
    "\n",
    "If no new data is needed, set the `GENERATE_DATA` variable to `False` to skip the data generation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "data_folder = \"data\"\n",
    "logs_folder = \"logs\"\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "if not os.path.exists(logs_folder):\n",
    "    os.makedirs(logs_folder)\n",
    "\n",
    "n_runs = 30\n",
    "\n",
    "GENERATE_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Submission Function\n",
    "\n",
    "We define a function `submit_job` that handles the submission of jobs to the scheduler. This function takes the number of nodes, the number of epochs and a job name as input parameters. It creates the necessary directories for storing data and logs, reads a template launch script, formats it with the provided parameters, writes the formatted script to a file, and submits the job using the `sbatch` command.\n",
    "\n",
    "Given the diverse types of jobs needed to be launched in this project not all the variables are used in the function. However, we keep them in the function definition to maintain consistency across the different job types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(launch_file, num_nodes, job_name, num_epochs):\n",
    "    num_tasks_per_node = 128\n",
    "\n",
    "    if num_tasks_per_node > 128:\n",
    "        print(\"The number of tasks per node should be less than or equal to 128\")\n",
    "        exit(1)\n",
    "\n",
    "    if not os.path.exists(f\"{data_folder}/{job_name}\"):\n",
    "        os.makedirs(f\"{data_folder}/{job_name}\")\n",
    "\n",
    "    if not os.path.exists(f\"{logs_folder}/{job_name}\"):\n",
    "        os.makedirs(f\"{logs_folder}/{job_name}\")\n",
    "\n",
    "    with open(launch_file, 'r') as file:\n",
    "        launch_script = file.read()\n",
    "\n",
    "    launch_script = launch_script.format(\n",
    "        num_nodes=num_nodes,\n",
    "        num_tasks_per_node=num_tasks_per_node,\n",
    "        current_dir=current_dir,\n",
    "        world_size=num_nodes*num_tasks_per_node,\n",
    "        num_epochs=num_epochs,\n",
    "        data_folder=f\"{data_folder}/{job_name}\",\n",
    "        logs_folder=f\"{logs_folder}/{job_name}\"\n",
    "    )\n",
    "\n",
    "    script_filename = f\"{logs_folder}/{job_name}/{launch_file.split('/')[-1]}\"\n",
    "    with open(script_filename, \"w\") as script_file:\n",
    "        script_file.write(launch_script)\n",
    "\n",
    "    os.system(f\"sbatch {script_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Test Functions\n",
    "\n",
    "In the following sections, we define functions to run different scalability tests. These functions will help us automate the process of submitting jobs for one million forecasters, strong scaling, and weak scaling tests. Each function will generate a unique job name, submit the job using the `submit_job` function, and return the job names for tracking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpi_test():\n",
    "    job_names = []\n",
    "    for i in range(n_runs):\n",
    "        run_dir = f\"{data_folder}/ten_nodes_test/run_{i}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.makedirs(run_dir)\n",
    "        \n",
    "        job_name = f\"/ten_nodes_test/run_{i}\"\n",
    "\n",
    "        submit_job(\"launchers/launch_cpu_batch.sh\", 1, job_name, 10)\n",
    "\n",
    "        job_names.append(job_name)\n",
    "\n",
    "    return job_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_test():\n",
    "    job_names = []\n",
    "    for i in range(n_runs):\n",
    "        run_dir = f\"{data_folder}/gpu_test/run_{i}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.makedirs(run_dir)\n",
    "        \n",
    "        job_name = f\"/gpu_test/run_{i}\"\n",
    "\n",
    "        submit_job(\"launchers/launch_gpu.sh\", 1, job_name, 10)\n",
    "\n",
    "        job_names.append(job_name)\n",
    "\n",
    "    return job_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_test():\n",
    "    job_names = []\n",
    "    for i in range(n_runs):\n",
    "        run_dir = f\"{data_folder}/baseline_test/run_{i}\"\n",
    "        if not os.path.exists(run_dir):\n",
    "            os.makedirs(run_dir)\n",
    "        \n",
    "        job_name = f\"/baseline_test/run_{i}\"\n",
    "\n",
    "        submit_job(\"launchers/launch_baseline.sh\", 1, job_name, 10)\n",
    "\n",
    "        job_names.append(job_name)\n",
    "\n",
    "    return job_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting for jobs\n",
    "\n",
    "Now we wait for all the jobs to complete, in the meantime the `tqdm` progress bar will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: access1.aion-cluster.uni.lux\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "print(f\"Hostname: {hostname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs_to_wait = []\n",
    "\n",
    "if GENERATE_DATA:\n",
    "    # if hostname contains \"aion\":\n",
    "    if \"aion\" in hostname:\n",
    "        all_jobs_to_wait.extend(baseline_test())\n",
    "        all_jobs_to_wait.extend(mpi_test())\n",
    "    elif \"iris\" in hostname:\n",
    "        all_jobs_to_wait.extend(gpu_test())\n",
    "\n",
    "    print(\"Waiting for joparallel_cnn/launchers/launch_gpu.shbs to finish...\")\n",
    "    print(all_jobs_to_wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for job_name in tqdm(all_jobs_to_wait):\n",
    "    while not os.path.exists(f\"{data_folder}/{job_name}/timings.txt\"):\n",
    "        time.sleep(10)  # Poll every 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Analysis\n",
    "\n",
    "In this section, we analyze the execution times 30 times of the baseline, the mpi and gpu version. We read the timing data from the generated files, calculate the mean and standard deviation of the execution times, and create a dataframe to summarize the results.\n",
    "\n",
    "The dataframe includes the following columns:\n",
    "- **Run**: The run identifier.\n",
    "- **Timing**: The total execution time for each run.\n",
    "- **CPU time**: The sum of CPU times across all ranks for each run.\n",
    "\n",
    "We then print the dataframe and the calculated mean and standard deviation of the execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_mean_and_std_of_times(job_name):\n",
    "    timings = []\n",
    "    cpu_times = []\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        with open(f\"{data_folder}/{job_name}/run_{i}/timings.txt\", \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            timings.append(float(lines[0].lstrip(\"Real time:\")))\n",
    "            cpu_times.append(float(lines[1].lstrip(\"CPU time:\")))\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "        'Run': [f'run_{i}' for i in range(n_runs)],\n",
    "        'Timing': timings,\n",
    "        'CPU Time': cpu_times\n",
    "    })\n",
    "\n",
    "    mean_timing = df['Timing'].mean()\n",
    "    std_timing = df['Timing'].std()\n",
    "\n",
    "    return df, mean_timing, std_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Test DataFrame:\n",
      "       Run     Timing    CPU Time\n",
      "0    run_0  7943.5663   9385.8839\n",
      "1    run_1  8657.1960   9906.1757\n",
      "2    run_2  6849.7040   8055.2187\n",
      "3    run_3  6726.0427   7879.7719\n",
      "4    run_4  7968.1344   9146.6158\n",
      "5    run_5  8686.6887   9970.8935\n",
      "6    run_6  6764.4210   7966.2162\n",
      "7    run_7  6695.2882   7853.4974\n",
      "8    run_8  7937.4752   9067.6058\n",
      "9    run_9  8851.9454  10114.9056\n",
      "10  run_10  6878.6781   8093.9334\n",
      "11  run_11  6734.8552   7911.2338\n",
      "12  run_12  8223.0523   9514.6220\n",
      "13  run_13  9060.9863  10384.1862\n",
      "14  run_14  6891.7856   8101.5388\n",
      "15  run_15  6729.4085   7898.5693\n",
      "16  run_16  7845.9373   9304.7396\n",
      "17  run_17  8792.5570  10070.7770\n",
      "18  run_18  6828.6254   8039.3488\n",
      "19  run_19  6702.5599   7865.9788\n",
      "20  run_20  7960.5973   9073.0003\n",
      "21  run_21  8526.8507   9813.1914\n",
      "22  run_22  6816.2103   8023.7662\n",
      "23  run_23  6722.7135   7891.0677\n",
      "24  run_24  8140.6264   9321.3822\n",
      "25  run_25  8773.0870  10080.3057\n",
      "26  run_26  6835.0856   8048.5783\n",
      "27  run_27  6707.6850   7880.9871\n",
      "28  run_28  8290.0598   9531.1791\n",
      "29  run_29  8712.3933   9993.1214\n",
      "Mean Timing: 7641.807213333333, Std Timing: 872.862890384193\n",
      "Baseline Test DataFrame:\n",
      "       Run      Timing    CPU Time\n",
      "0    run_0  43308.3763  42427.3452\n",
      "1    run_1  44173.3077  43330.2025\n",
      "2    run_2  43290.2155  42348.9850\n",
      "3    run_3  43632.4923  42639.8734\n",
      "4    run_4  44129.9207  43274.5405\n",
      "5    run_5  43351.6610  42424.2853\n",
      "6    run_6  43687.2153  42761.6179\n",
      "7    run_7  44135.3159  43254.1873\n",
      "8    run_8  43828.6836  42945.5802\n",
      "9    run_9  43500.8068  42602.6280\n",
      "10  run_10  43525.9757  42636.9675\n",
      "11  run_11  44130.3961  43216.8153\n",
      "12  run_12  43699.4486  42852.5948\n",
      "13  run_13  43301.8517  42394.8201\n",
      "14  run_14  43841.3954  42936.3290\n",
      "15  run_15  44205.1008  43335.5100\n",
      "16  run_16  43408.5944  42462.9297\n",
      "17  run_17  43668.8840  42689.6231\n",
      "18  run_18  44225.0879  43319.6791\n",
      "19  run_19  43925.1335  43064.2975\n",
      "20  run_20  43838.4905  42935.2644\n",
      "21  run_21  43131.9528  42186.3583\n",
      "22  run_22  43867.1303  42996.0202\n",
      "23  run_23  44301.4064  43440.8759\n",
      "24  run_24  43488.0185  42618.9917\n",
      "25  run_25  43911.8905  43030.5097\n",
      "26  run_26  43362.1507  42473.0398\n",
      "27  run_27  43775.0351  42911.2801\n",
      "28  run_28  43166.1217  42274.3562\n",
      "29  run_29  43580.5295  42677.2430\n",
      "Mean Timing: 43713.08630666667, Std Timing: 342.3370389340838\n",
      "Baseline Test DataFrame:\n",
      "       Run      Timing    CPU Time\n",
      "0    run_0  59953.9268  59626.1780\n",
      "1    run_1  59498.2855  59169.6150\n",
      "2    run_2  57583.6515  57287.0404\n",
      "3    run_3  57354.2793  57064.5333\n",
      "4    run_4  56100.8699  55812.5782\n",
      "5    run_5  56414.2444  55802.1984\n",
      "6    run_6  56498.1205  56048.4085\n",
      "7    run_7  56524.9453  56079.1701\n",
      "8    run_8  63342.2329  63073.4411\n",
      "9    run_9  56091.7165  55601.3059\n",
      "10  run_10  63826.0238  63555.4770\n",
      "11  run_11  63637.7356  63328.5706\n",
      "12  run_12  56647.0640  56199.8483\n",
      "13  run_13  58989.4395  58478.2671\n",
      "14  run_14  59038.8935  58567.9311\n",
      "15  run_15  59147.9220  58682.1893\n",
      "16  run_16  56570.5421  56054.7985\n",
      "17  run_17  58087.2695  57609.2123\n",
      "18  run_18  58131.0811  57651.4493\n",
      "19  run_19  58753.4428  58124.0092\n",
      "20  run_20  61818.8148  61279.2703\n",
      "21  run_21  63042.3357  62493.6732\n",
      "22  run_22  63499.0451  62949.9965\n",
      "23  run_23  63436.5757  62880.5303\n",
      "24  run_24  61988.6086  61466.2069\n",
      "25  run_25  62935.1330  62399.0033\n",
      "26  run_26  62942.2352  62406.8561\n",
      "27  run_27  63225.7314  62688.5165\n",
      "28  run_28  59694.2885  59203.0625\n",
      "29  run_29  57204.3233  56740.7023\n",
      "Mean Timing: 59732.62592666666, Std Timing: 2805.1824158587883\n",
      "Speedup GPU: 7.81655755754292\n",
      "Speedup CPU: 1.366470111664407\n"
     ]
    }
   ],
   "source": [
    "# Get the dataframe for GPU test\n",
    "df_gpu, mean_timing_gpu, std_timing_gpu = get_mean_and_std_of_times(\"gpu_test\")\n",
    "print(\"GPU Test DataFrame:\")\n",
    "print(df_gpu)\n",
    "print(f\"Mean Timing: {mean_timing_gpu}, Std Timing: {std_timing_gpu}\")\n",
    "\n",
    "# Get the dataframe for Baseline test\n",
    "df_cpu, mean_timing_cpu, std_timing_cpu = get_mean_and_std_of_times(\"ten_nodes_test\")\n",
    "print(\"Baseline Test DataFrame:\")\n",
    "print(df_cpu)\n",
    "print(f\"Mean Timing: {mean_timing_cpu}, Std Timing: {std_timing_cpu}\")\n",
    "\n",
    "# Get the dataframe for Baseline test\n",
    "df_baseline, mean_timing_baseline, std_timing_baseline = get_mean_and_std_of_times(\"baseline_test\")\n",
    "print(\"Baseline Test DataFrame:\")\n",
    "print(df_baseline)\n",
    "print(f\"Mean Timing: {mean_timing_baseline}, Std Timing: {std_timing_baseline}\")\n",
    "\n",
    "# print mean speedup\n",
    "speedup = mean_timing_baseline / mean_timing_gpu\n",
    "print(f\"Speedup GPU: {speedup}\")\n",
    "\n",
    "# print mean speedup\n",
    "speedup = mean_timing_baseline / mean_timing_cpu\n",
    "print(f\"Speedup CPU: {speedup}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speedup analysis\n",
    "\n",
    "### MPI\n",
    "\n",
    "The speedup obtained for the MPI implementation is 1.36. This indicates that the MPI implementation is 1.36 times faster than the baseline CPU implementation.\n",
    "\n",
    "The speedup value suggests that the parallelization using MPI has effectively reduced the execution time, although the improvement is not substantial. This moderate speedup could be due to the overhead associated with communication between nodes and the relatively small problem size, which may not fully leverage the benefits of parallel execution.\n",
    "\n",
    "### GPU\n",
    "\n",
    "The speedup obtained for the GPU implementation is 7.81. This indicates that the GPU implementation is 7.81 times faster than the baseline CPU implementation. The significant speedup demonstrates the effectiveness of leveraging GPU acceleration for this workload.\n",
    "\n",
    "GPUs are well-suited for parallel processing tasks, and their ability to handle multiple operations simultaneously has resulted in a substantial reduction in execution time compared to the CPU implementation. This highlights the potential benefits of utilizing GPU resources for computationally intensive tasks in high-performance computing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The results of this project demonstrate the potential benefits of utilizing parallel computing techniques, such as MPI and GPU acceleration, for high-performance computing applications. While the MPI implementation provided a moderate improvement, the GPU implementation significantly outperformed the baseline CPU implementation, showcasing the advantages of GPU acceleration for computationally intensive tasks.\n",
    "\n",
    "Future work could involve optimizing the MPI implementation to reduce communication overhead and exploring hybrid approaches that combine MPI and GPU acceleration to further enhance performance. Additionally, scaling the problem size and testing on larger datasets could provide more insights into the scalability and efficiency of the parallel implementations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
